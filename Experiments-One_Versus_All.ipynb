{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "from raytracerthing import RayTracerThing, Activations\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "digits.keys()\n",
    "\n",
    "y = digits['target']\n",
    "# y = np.array(y==1, dtype=int)\n",
    "ones = y[digits['target'] == target]\n",
    "not_ones = y[digits['target'] != target]\n",
    "\n",
    "n = min(len(ones), len(not_ones))\n",
    "\n",
    "y = np.concatenate([np.ones(n, dtype=int), np.zeros(n, dtype=int)])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 (8, 8)\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = digits['images']\n",
    "X = np.concatenate([X[digits['target'] == target][:n], X[digits['target'] != target][:n]])\n",
    "\n",
    "X = X / X.max()\n",
    "N = X.shape[0]\n",
    "image_shape = X.shape[1:]\n",
    "\n",
    "print(N, image_shape)\n",
    "\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([178, 178]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 0 ns, total: 31.2 ms\n",
      "Wall time: 31.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = RayTracerThing(input_shape=image_shape,\n",
    "                     hidden_layer_shape=image_shape, \n",
    "                     n_layers=3,\n",
    "                     n_classes=2,\n",
    "                     activation_func=lambda x: Activations.sigmoid(x, alpha=10),\n",
    "                     loss_func=torch.nn.functional.binary_cross_entropy,\n",
    "                     learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2000 - train_loss: 0.7739 - train_acc: 0.7333 - val_loss: 0.5804 - val_acc: 0.7018\n",
      "Epoch 2 of 2000 - train_loss: 0.3988 - train_acc: 0.7958 - val_loss: 0.4580 - val_acc: 0.7544\n",
      "Epoch 3 of 2000 - train_loss: 0.3309 - train_acc: 0.8542 - val_loss: 0.4006 - val_acc: 0.8070\n",
      "Epoch 4 of 2000 - train_loss: 0.3048 - train_acc: 0.8667 - val_loss: 0.3632 - val_acc: 0.8421\n",
      "Epoch 5 of 2000 - train_loss: 0.2853 - train_acc: 0.8667 - val_loss: 0.3390 - val_acc: 0.8421\n",
      "Epoch 6 of 2000 - train_loss: 0.2649 - train_acc: 0.8917 - val_loss: 0.3273 - val_acc: 0.8596\n",
      "Epoch 7 of 2000 - train_loss: 0.2510 - train_acc: 0.8958 - val_loss: 0.3225 - val_acc: 0.8596\n",
      "Epoch 8 of 2000 - train_loss: 0.2444 - train_acc: 0.9000 - val_loss: 0.3186 - val_acc: 0.8596\n",
      "Epoch 9 of 2000 - train_loss: 0.2388 - train_acc: 0.9000 - val_loss: 0.3145 - val_acc: 0.8772\n",
      "Epoch 10 of 2000 - train_loss: 0.2330 - train_acc: 0.9000 - val_loss: 0.3103 - val_acc: 0.8772\n",
      "Epoch 11 of 2000 - train_loss: 0.2277 - train_acc: 0.9042 - val_loss: 0.3087 - val_acc: 0.8772\n",
      "Epoch 12 of 2000 - train_loss: 0.2251 - train_acc: 0.9083 - val_loss: 0.3083 - val_acc: 0.8772\n",
      "Epoch 13 of 2000 - train_loss: 0.2237 - train_acc: 0.9083 - val_loss: 0.3080 - val_acc: 0.8772\n",
      "Epoch 14 of 2000 - train_loss: 0.2226 - train_acc: 0.9083 - val_loss: 0.3077 - val_acc: 0.8772\n",
      "Epoch 15 of 2000 - train_loss: 0.2216 - train_acc: 0.9083 - val_loss: 0.3072 - val_acc: 0.8772\n",
      "Epoch 16 of 2000 - train_loss: 0.2205 - train_acc: 0.9083 - val_loss: 0.3067 - val_acc: 0.8772\n",
      "Epoch 17 of 2000 - train_loss: 0.2191 - train_acc: 0.9083 - val_loss: 0.3060 - val_acc: 0.8772\n",
      "Epoch 18 of 2000 - train_loss: 0.2176 - train_acc: 0.9083 - val_loss: 0.3052 - val_acc: 0.8947\n",
      "Epoch 19 of 2000 - train_loss: 0.2158 - train_acc: 0.9083 - val_loss: 0.3041 - val_acc: 0.8947\n",
      "Epoch 20 of 2000 - train_loss: 0.2136 - train_acc: 0.9167 - val_loss: 0.3027 - val_acc: 0.8947\n",
      "Epoch 21 of 2000 - train_loss: 0.2109 - train_acc: 0.9208 - val_loss: 0.3009 - val_acc: 0.8947\n",
      "Epoch 22 of 2000 - train_loss: 0.2077 - train_acc: 0.9250 - val_loss: 0.2989 - val_acc: 0.8947\n",
      "Epoch 23 of 2000 - train_loss: 0.2042 - train_acc: 0.9292 - val_loss: 0.2973 - val_acc: 0.8947\n",
      "Epoch 24 of 2000 - train_loss: 0.2020 - train_acc: 0.9292 - val_loss: 0.2965 - val_acc: 0.8947\n",
      "Epoch 25 of 2000 - train_loss: 0.2004 - train_acc: 0.9333 - val_loss: 0.2955 - val_acc: 0.8947\n",
      "Epoch 26 of 2000 - train_loss: 0.1988 - train_acc: 0.9375 - val_loss: 0.2943 - val_acc: 0.8947\n",
      "Epoch 27 of 2000 - train_loss: 0.1967 - train_acc: 0.9375 - val_loss: 0.2926 - val_acc: 0.8947\n",
      "Epoch 28 of 2000 - train_loss: 0.1939 - train_acc: 0.9375 - val_loss: 0.2903 - val_acc: 0.8947\n",
      "Epoch 29 of 2000 - train_loss: 0.1900 - train_acc: 0.9417 - val_loss: 0.2872 - val_acc: 0.8947\n",
      "Epoch 30 of 2000 - train_loss: 0.1851 - train_acc: 0.9458 - val_loss: 0.2842 - val_acc: 0.8772\n",
      "Epoch 31 of 2000 - train_loss: 0.1812 - train_acc: 0.9542 - val_loss: 0.2838 - val_acc: 0.8772\n",
      "Epoch 32 of 2000 - train_loss: 0.1803 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 33 of 2000 - train_loss: 0.1799 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 34 of 2000 - train_loss: 0.1797 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 35 of 2000 - train_loss: 0.1796 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 36 of 2000 - train_loss: 0.1795 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 37 of 2000 - train_loss: 0.1794 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 38 of 2000 - train_loss: 0.1794 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 39 of 2000 - train_loss: 0.1793 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 40 of 2000 - train_loss: 0.1792 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 41 of 2000 - train_loss: 0.1791 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 42 of 2000 - train_loss: 0.1791 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 43 of 2000 - train_loss: 0.1790 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 44 of 2000 - train_loss: 0.1789 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 45 of 2000 - train_loss: 0.1789 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 46 of 2000 - train_loss: 0.1788 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 47 of 2000 - train_loss: 0.1787 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 48 of 2000 - train_loss: 0.1786 - train_acc: 0.9542 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 49 of 2000 - train_loss: 0.1786 - train_acc: 0.9542 - val_loss: 0.2838 - val_acc: 0.8772\n",
      "Epoch 50 of 2000 - train_loss: 0.1785 - train_acc: 0.9542 - val_loss: 0.2838 - val_acc: 0.8772\n",
      "Epoch 51 of 2000 - train_loss: 0.1784 - train_acc: 0.9542 - val_loss: 0.2838 - val_acc: 0.8772\n",
      "Epoch 52 of 2000 - train_loss: 0.1783 - train_acc: 0.9542 - val_loss: 0.2838 - val_acc: 0.8772\n",
      "Reducing learning rate to 0.1000\n",
      "Epoch 53 of 2000 - train_loss: 0.1570 - train_acc: 0.9542 - val_loss: 0.2673 - val_acc: 0.8947\n",
      "Epoch 54 of 2000 - train_loss: 0.1504 - train_acc: 0.9500 - val_loss: 0.2664 - val_acc: 0.9123\n",
      "Epoch 55 of 2000 - train_loss: 0.1495 - train_acc: 0.9500 - val_loss: 0.2657 - val_acc: 0.9123\n",
      "Epoch 56 of 2000 - train_loss: 0.1492 - train_acc: 0.9500 - val_loss: 0.2654 - val_acc: 0.9123\n",
      "Epoch 57 of 2000 - train_loss: 0.1490 - train_acc: 0.9500 - val_loss: 0.2651 - val_acc: 0.9123\n",
      "Epoch 58 of 2000 - train_loss: 0.1488 - train_acc: 0.9500 - val_loss: 0.2648 - val_acc: 0.9123\n",
      "Epoch 59 of 2000 - train_loss: 0.1487 - train_acc: 0.9500 - val_loss: 0.2645 - val_acc: 0.9123\n",
      "Epoch 60 of 2000 - train_loss: 0.1487 - train_acc: 0.9500 - val_loss: 0.2643 - val_acc: 0.9123\n",
      "Epoch 61 of 2000 - train_loss: 0.1486 - train_acc: 0.9500 - val_loss: 0.2640 - val_acc: 0.9123\n",
      "Epoch 62 of 2000 - train_loss: 0.1485 - train_acc: 0.9500 - val_loss: 0.2638 - val_acc: 0.9123\n",
      "Epoch 63 of 2000 - train_loss: 0.1485 - train_acc: 0.9500 - val_loss: 0.2636 - val_acc: 0.9123\n",
      "Epoch 64 of 2000 - train_loss: 0.1484 - train_acc: 0.9500 - val_loss: 0.2634 - val_acc: 0.9123\n",
      "Epoch 65 of 2000 - train_loss: 0.1484 - train_acc: 0.9500 - val_loss: 0.2632 - val_acc: 0.9123\n",
      "Epoch 66 of 2000 - train_loss: 0.1483 - train_acc: 0.9500 - val_loss: 0.2630 - val_acc: 0.9123\n",
      "Epoch 67 of 2000 - train_loss: 0.1482 - train_acc: 0.9500 - val_loss: 0.2629 - val_acc: 0.9123\n",
      "Epoch 68 of 2000 - train_loss: 0.1482 - train_acc: 0.9500 - val_loss: 0.2627 - val_acc: 0.9123\n",
      "Epoch 69 of 2000 - train_loss: 0.1482 - train_acc: 0.9500 - val_loss: 0.2625 - val_acc: 0.9123\n",
      "Epoch 70 of 2000 - train_loss: 0.1481 - train_acc: 0.9500 - val_loss: 0.2624 - val_acc: 0.9123\n",
      "Epoch 71 of 2000 - train_loss: 0.1480 - train_acc: 0.9500 - val_loss: 0.2623 - val_acc: 0.9123\n",
      "Epoch 72 of 2000 - train_loss: 0.1480 - train_acc: 0.9500 - val_loss: 0.2622 - val_acc: 0.9123\n",
      "Epoch 73 of 2000 - train_loss: 0.1480 - train_acc: 0.9500 - val_loss: 0.2621 - val_acc: 0.9123\n",
      "Epoch 74 of 2000 - train_loss: 0.1479 - train_acc: 0.9500 - val_loss: 0.2621 - val_acc: 0.9123\n",
      "Epoch 75 of 2000 - train_loss: 0.1479 - train_acc: 0.9500 - val_loss: 0.2620 - val_acc: 0.9123\n",
      "Epoch 76 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2620 - val_acc: 0.9123\n",
      "Epoch 77 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2619 - val_acc: 0.9123\n",
      "Epoch 78 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2619 - val_acc: 0.9123\n",
      "Epoch 79 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2619 - val_acc: 0.9123\n",
      "Epoch 80 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2618 - val_acc: 0.9123\n",
      "Epoch 81 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2618 - val_acc: 0.9123\n",
      "Epoch 82 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2618 - val_acc: 0.9123\n",
      "Epoch 83 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2617 - val_acc: 0.9123\n",
      "Epoch 84 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2617 - val_acc: 0.9123\n",
      "Epoch 85 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2617 - val_acc: 0.9123\n",
      "Epoch 86 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2617 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2616 - val_acc: 0.9123\n",
      "Epoch 88 of 2000 - train_loss: 0.1479 - train_acc: 0.9542 - val_loss: 0.2616 - val_acc: 0.9123\n",
      "Epoch 89 of 2000 - train_loss: 0.1478 - train_acc: 0.9542 - val_loss: 0.2616 - val_acc: 0.9123\n",
      "Epoch 90 of 2000 - train_loss: 0.1478 - train_acc: 0.9542 - val_loss: 0.2616 - val_acc: 0.9123\n",
      "Epoch 91 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2615 - val_acc: 0.9123\n",
      "Epoch 92 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2615 - val_acc: 0.9123\n",
      "Epoch 93 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2615 - val_acc: 0.9123\n",
      "Epoch 94 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2615 - val_acc: 0.9123\n",
      "Epoch 95 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2615 - val_acc: 0.9123\n",
      "Epoch 96 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2614 - val_acc: 0.9123\n",
      "Epoch 97 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2614 - val_acc: 0.9123\n",
      "Epoch 98 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2614 - val_acc: 0.9123\n",
      "Epoch 99 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2614 - val_acc: 0.9123\n",
      "Epoch 100 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2614 - val_acc: 0.9123\n",
      "Epoch 101 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2613 - val_acc: 0.9123\n",
      "Epoch 102 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2613 - val_acc: 0.9123\n",
      "Epoch 103 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2613 - val_acc: 0.9123\n",
      "Epoch 104 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2613 - val_acc: 0.9123\n",
      "Epoch 105 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2613 - val_acc: 0.9123\n",
      "Epoch 106 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2613 - val_acc: 0.9123\n",
      "Epoch 107 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 108 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 109 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 110 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 111 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 112 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 113 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 114 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2612 - val_acc: 0.9123\n",
      "Epoch 115 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 116 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 117 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 118 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 119 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 120 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 121 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 122 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 123 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2611 - val_acc: 0.9123\n",
      "Epoch 124 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 125 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 126 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 127 of 2000 - train_loss: 0.1478 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 128 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 129 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 130 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 131 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 132 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 133 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 134 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 135 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 136 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 137 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 138 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 139 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 140 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 141 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 142 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 143 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 144 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 145 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 146 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 147 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 148 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 149 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 150 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 151 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 152 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 153 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 154 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 155 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 156 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 157 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2609 - val_acc: 0.9123\n",
      "Epoch 158 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 159 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 160 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 161 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 162 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 163 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 164 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 165 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 166 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 167 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 168 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 169 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 170 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 171 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 172 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 173 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 174 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 175 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 176 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 177 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 178 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 179 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 180 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 181 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 182 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 183 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 184 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 185 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 186 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 187 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 188 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 189 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 190 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 191 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 192 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 193 of 2000 - train_loss: 0.1477 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 194 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 195 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 196 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 197 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 198 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 199 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 200 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 201 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 202 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 203 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 204 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 205 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 206 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 207 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 208 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 209 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 210 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 211 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 212 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 213 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 214 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 215 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 216 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 217 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 218 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 219 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 220 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 221 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 222 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 223 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 224 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Epoch 225 of 2000 - train_loss: 0.1476 - train_acc: 0.9500 - val_loss: 0.2608 - val_acc: 0.9123\n",
      "Reducing learning rate to 0.0100\n",
      "Epoch 226 of 2000 - train_loss: 0.1471 - train_acc: 0.9583 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 227 of 2000 - train_loss: 0.1469 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 228 of 2000 - train_loss: 0.1468 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 229 of 2000 - train_loss: 0.1468 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 230 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 231 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 232 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 233 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 234 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 235 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 236 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 237 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 238 of 2000 - train_loss: 0.1467 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 239 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2605 - val_acc: 0.9123\n",
      "Epoch 240 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2604 - val_acc: 0.9123\n",
      "Epoch 241 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2604 - val_acc: 0.9123\n",
      "Epoch 242 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2604 - val_acc: 0.9123\n",
      "Epoch 243 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2604 - val_acc: 0.9123\n",
      "Epoch 244 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2604 - val_acc: 0.9123\n",
      "Epoch 245 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2604 - val_acc: 0.9123\n",
      "Epoch 246 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 247 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 248 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 249 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 250 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 251 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 252 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 253 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2603 - val_acc: 0.9123\n",
      "Epoch 254 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 255 of 2000 - train_loss: 0.1466 - train_acc: 0.9542 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 256 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 258 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 259 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 260 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 261 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 262 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 263 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2602 - val_acc: 0.9123\n",
      "Epoch 264 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 265 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 266 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 267 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 268 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 269 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 270 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 271 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 272 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 273 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 274 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2601 - val_acc: 0.9123\n",
      "Epoch 275 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 276 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 277 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 278 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 279 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 280 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 281 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 282 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 283 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 284 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 285 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 286 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 287 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 288 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 289 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 290 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 291 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 292 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 293 of 2000 - train_loss: 0.1466 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 294 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 295 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2600 - val_acc: 0.9123\n",
      "Epoch 296 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 297 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 298 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 299 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 300 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 301 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 302 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 303 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 304 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 305 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 306 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 307 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 308 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 309 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 310 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 311 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 312 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 313 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 314 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 315 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 316 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 317 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 318 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 319 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 320 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 321 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 322 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 323 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 324 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 325 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 326 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 327 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 328 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 329 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 330 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 331 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 332 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 333 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 334 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 335 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 336 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 337 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 338 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 339 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 340 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 341 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 342 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 343 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 344 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 345 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 346 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 347 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 348 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 349 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 350 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 351 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 352 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 353 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 354 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 355 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 356 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 357 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 358 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 359 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 360 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 361 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 362 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9123\n",
      "Epoch 363 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 364 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 365 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 366 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 367 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 368 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 369 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 370 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 371 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 372 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 373 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 374 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 375 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 376 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 377 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 378 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 379 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 380 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 381 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 382 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 383 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 384 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 385 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 386 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 387 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 388 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 389 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 390 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 391 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 392 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 393 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 394 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 395 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 396 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 397 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 398 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 399 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 400 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 401 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 402 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 403 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 404 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 405 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 406 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 407 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 408 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 409 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 410 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 411 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 412 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 413 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 414 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 415 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 416 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 417 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 418 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 419 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 420 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 421 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 422 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 423 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 424 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 425 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 426 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 428 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 429 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 430 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 431 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 432 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 433 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 434 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 435 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 436 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 437 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 438 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 439 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 440 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 441 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 442 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 443 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 444 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 445 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 446 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 447 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 448 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 449 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 450 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 451 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 452 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 453 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 454 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 455 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 456 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 457 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 458 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 459 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 460 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 461 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 462 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 463 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 464 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 465 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 466 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 467 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 468 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 469 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 470 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 471 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 472 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 473 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 474 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 475 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 476 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 477 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 478 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 479 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 480 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 481 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 482 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 483 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 484 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 485 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 486 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 487 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 488 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 489 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 490 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 491 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 492 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 493 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 494 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 495 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 496 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 497 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 498 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 499 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 500 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 501 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 502 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 503 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 504 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 505 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 506 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 507 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 508 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 509 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 510 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2598 - val_acc: 0.9123\n",
      "Epoch 511 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 512 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 513 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 514 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 515 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 516 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 517 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 518 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 519 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 520 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 521 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 522 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 523 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 524 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 525 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 526 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 527 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 528 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 529 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 530 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 531 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 532 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 533 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 534 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 535 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 536 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 537 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 538 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 539 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 540 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 541 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 542 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 543 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 544 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 545 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 546 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 547 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 548 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 549 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 550 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 551 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 552 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 553 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 554 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 555 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 556 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 557 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 558 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 559 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 560 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 561 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 562 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 563 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 564 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 565 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 566 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 567 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 568 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 569 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 570 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 571 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 572 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 573 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 574 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 575 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 576 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 577 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 578 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 579 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 580 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 581 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 582 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 583 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 584 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 585 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 586 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 587 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 588 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 589 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 590 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 591 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 592 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 593 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 594 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 595 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 596 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 597 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 598 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 599 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 600 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 601 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 602 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 603 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 604 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 605 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 606 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 607 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 608 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 609 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 610 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 611 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 612 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 613 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 614 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 615 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 616 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 617 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 618 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 619 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 620 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 621 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 622 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 623 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 624 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 625 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 626 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 627 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 628 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 629 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 630 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 631 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 632 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 633 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 634 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 635 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 636 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 637 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 638 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 639 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 640 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 641 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 642 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 643 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 644 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 645 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 646 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 647 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 648 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 649 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 650 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 651 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 652 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 653 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 654 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 655 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 656 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 657 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 658 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 659 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 660 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 661 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 662 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 663 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 664 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 665 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 666 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 667 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 668 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 669 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 670 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 671 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 672 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 673 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 674 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 675 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 676 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 677 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 678 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 679 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 680 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 681 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 682 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 683 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 684 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 685 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 686 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 687 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 688 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 689 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 690 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 691 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 692 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 693 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 694 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 695 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 696 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 697 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 698 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 699 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 700 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 701 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 702 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 703 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 704 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 705 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 706 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 707 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2597 - val_acc: 0.9123\n",
      "Epoch 708 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 709 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 710 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 711 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 712 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 713 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 714 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 715 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 716 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 717 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 718 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 719 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 720 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 721 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 722 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 723 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 724 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 725 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 726 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 727 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 728 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 729 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 730 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 731 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 732 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 733 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 734 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 735 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 736 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 737 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 738 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 739 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 740 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 741 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 742 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 743 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 744 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 745 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 746 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 747 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 748 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 749 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 750 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 751 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 752 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 753 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 754 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 755 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 756 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 757 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 758 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 759 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 760 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 761 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 762 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 763 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 764 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 765 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 766 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 768 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 769 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 770 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 771 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 772 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 773 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 774 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 775 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 776 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 777 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 778 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 779 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 780 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 781 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 782 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 783 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 784 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 785 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 786 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 787 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 788 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 789 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 790 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 791 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 792 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 793 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 794 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 795 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 796 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 797 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 798 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 799 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 800 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 801 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 802 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 803 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 804 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 805 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 806 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 807 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 808 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 809 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 810 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 811 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 812 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 813 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 814 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 815 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 816 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 817 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 818 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 819 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 820 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 821 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 822 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 823 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 824 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 825 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 826 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 827 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 828 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 829 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 830 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 831 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 832 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 833 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 834 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 835 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 836 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 837 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 838 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 839 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 840 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 841 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 842 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 843 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 844 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 845 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 846 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 847 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 848 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 849 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 850 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 851 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 852 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 853 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 854 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 855 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 856 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 857 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 858 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 859 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 860 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 861 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 862 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 863 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 864 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 865 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 866 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 867 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 868 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 869 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 870 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 871 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 872 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 873 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 874 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 875 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 876 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 877 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 878 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 879 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 880 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 881 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 882 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 883 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 884 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 885 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 886 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 887 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 888 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 889 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 890 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 891 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 892 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 893 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 894 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 895 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 896 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 897 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 898 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 899 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 900 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 901 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 902 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 903 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 904 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 905 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 906 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 907 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 908 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 909 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 910 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 911 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 912 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 913 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 914 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 915 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 916 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 917 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 918 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 919 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 920 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 921 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 922 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 923 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 924 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 925 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 926 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 927 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 928 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 929 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 930 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 931 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 932 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 933 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 934 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 935 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 936 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 937 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 938 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 939 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 940 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 941 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 942 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 943 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 944 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 945 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 946 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 947 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 948 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 949 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 950 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 951 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 952 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 953 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 954 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 955 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 956 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 957 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 958 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 959 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 960 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 961 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 962 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 963 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 964 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 965 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 966 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 967 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 968 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 969 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 970 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 971 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 972 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 973 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 974 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 975 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 976 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 977 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 978 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 979 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 980 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 981 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2596 - val_acc: 0.9123\n",
      "Epoch 982 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 983 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 984 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 985 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 986 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 987 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 988 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 989 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 990 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 991 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 992 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 993 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 994 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 995 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 996 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 997 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 998 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 999 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1000 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1001 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1002 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1003 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1004 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1005 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1006 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1007 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1008 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1009 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1010 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1011 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1012 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1013 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1014 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1015 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1016 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1017 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1018 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1019 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1020 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1021 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1022 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1023 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1024 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1025 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1026 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1027 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1028 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1029 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1030 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1031 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1032 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1033 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1034 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1035 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1036 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1037 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1038 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1039 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1040 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1041 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1042 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1043 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1044 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1045 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1046 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1047 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1048 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1049 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1050 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1051 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1052 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1053 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1054 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1055 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1056 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1057 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1058 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1059 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1060 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1061 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1062 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1063 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1064 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1065 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1066 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1067 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1068 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1069 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1070 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1071 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1072 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1073 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1074 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1075 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1076 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1077 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1078 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1079 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1080 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1081 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1082 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1083 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1084 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1085 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1086 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1087 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1088 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1089 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1090 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1091 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1092 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1093 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1094 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1095 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1096 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1097 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1098 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1099 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1100 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1101 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1102 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1103 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1104 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1105 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1106 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1107 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1108 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1109 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1110 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1111 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1112 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1113 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1114 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1115 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1116 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1117 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1118 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1119 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1120 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1121 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1122 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1123 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1124 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1125 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1126 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1127 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1128 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1129 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1130 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1131 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1132 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1133 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1134 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1135 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1136 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1137 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1138 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1139 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1140 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1141 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1142 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1143 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1144 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1145 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1146 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1147 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1148 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1149 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1150 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1151 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1152 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1153 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1154 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1155 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1156 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1157 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1158 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1159 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1160 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1161 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1162 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1163 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1164 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1165 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1166 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1167 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1168 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1169 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1170 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1171 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1172 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1173 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1174 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1175 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1176 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1177 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1178 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1179 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1180 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1181 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1182 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1183 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1184 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1185 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1186 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1187 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1188 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1189 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1190 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1191 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1192 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1193 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1194 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1195 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1196 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1197 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1198 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1199 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1200 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1201 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1202 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1203 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1204 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1205 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1206 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1207 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1208 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1209 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1210 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1211 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1212 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1213 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1214 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1215 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1216 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1217 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1218 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1219 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1220 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1221 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1222 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1223 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1224 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1225 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1226 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1227 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1228 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1229 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1230 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1231 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1232 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1233 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1234 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1235 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1236 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1237 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1238 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1239 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1240 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1241 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1242 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1243 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1244 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1245 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1246 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1247 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1248 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1249 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1250 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1251 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1252 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1253 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1254 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1255 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1256 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1257 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1258 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1259 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1260 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1261 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1262 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1263 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1264 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1265 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1266 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1267 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1268 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1269 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1270 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1271 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1272 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1273 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1274 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1275 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1276 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1277 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1278 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1279 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1280 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1281 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1282 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1283 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1284 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1285 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1286 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1287 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1288 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1289 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1290 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1291 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1292 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1293 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1294 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1295 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1296 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1297 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1298 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1299 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1300 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1301 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1302 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1303 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1304 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1305 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1306 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1307 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1308 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1309 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1310 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1311 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1312 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1313 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1314 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1315 of 2000 - train_loss: 0.1465 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1316 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1317 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1318 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1319 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1320 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1321 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1322 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1323 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1324 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1325 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1326 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1327 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1328 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1329 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1330 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1331 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1332 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1333 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1334 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1335 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1336 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1337 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1338 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1339 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1340 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1341 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1342 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1343 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1344 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1345 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1346 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1347 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1348 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1349 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1350 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1351 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1352 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1353 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1354 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1355 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1356 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1357 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1358 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1359 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1360 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1361 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1362 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1363 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1364 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1365 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1366 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1367 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1368 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1369 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1370 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1371 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1372 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1373 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1374 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1375 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1376 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1377 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1378 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1379 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1380 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1381 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1382 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1383 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1384 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1385 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1386 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1387 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1388 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1389 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1390 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1391 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1392 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1393 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1394 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1395 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1396 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1397 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1398 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1399 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1400 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1401 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1402 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1403 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1404 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1405 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1406 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1407 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1408 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1409 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1410 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1411 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1412 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1413 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1414 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1415 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1416 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2595 - val_acc: 0.9123\n",
      "Epoch 1417 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1418 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1419 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1420 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1421 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1422 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1423 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1424 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1425 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1426 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1427 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1428 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1429 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1430 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1431 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1432 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1433 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1434 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1435 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1436 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1437 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1438 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1439 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1440 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1441 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1442 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1443 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1444 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1445 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1446 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1447 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1448 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1449 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1450 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1451 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1452 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1453 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1454 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1455 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1456 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1457 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1458 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1459 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1460 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1461 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1462 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1463 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1464 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1465 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1466 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1467 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1468 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1469 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1470 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1471 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1472 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1473 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1474 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1475 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1476 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1477 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1478 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1479 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1480 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1481 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1482 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1483 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1484 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1485 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1486 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1487 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1488 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1489 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1490 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1491 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1492 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1493 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1494 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1495 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1496 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1497 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1498 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1499 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1500 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1501 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1502 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1503 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1504 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1505 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1506 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1507 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1508 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1509 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1510 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1511 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1512 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1513 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1514 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1515 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1516 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1517 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1518 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1519 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1520 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1521 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1522 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1523 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1524 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1525 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1526 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1527 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1528 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1529 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1530 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1531 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1532 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1533 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1534 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1535 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1536 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1537 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1538 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1539 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1540 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1541 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1542 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1543 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1544 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1545 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1546 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1547 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1548 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1549 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1550 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1551 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1552 of 2000 - train_loss: 0.1464 - train_acc: 0.9542 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Reducing learning rate to 0.0010\n",
      "Epoch 1553 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1554 of 2000 - train_loss: 0.1464 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1555 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1556 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1557 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1558 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1559 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1560 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1561 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1562 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1563 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1564 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1565 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Reducing learning rate to 0.0001\n",
      "Epoch 1566 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1567 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1568 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1569 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1570 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1571 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1572 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1573 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1574 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1575 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "Epoch 1576 of 2000 - train_loss: 0.1463 - train_acc: 0.9583 - val_loss: 0.2594 - val_acc: 0.9123\n",
      "\n",
      "Stopping early.\n",
      "\n",
      "CPU times: user 3min 1s, sys: 3.77 s, total: 3min 5s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train, batch_size=16, n_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9722)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97        36\n",
      "          1       0.97      0.97      0.97        36\n",
      "\n",
      "avg / total       0.97      0.97      0.97        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues')\n",
    "\n",
    "cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "cm.xaxis.tick_top()\n",
    "cm.xaxis.set_label_position('top')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 72 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 6\n",
    "n_cols = 6\n",
    "n_total = n_rows * n_cols\n",
    "n_total = min(n_total, N)\n",
    "\n",
    "width_ratio = 2.7\n",
    "height_ratio = 2\n",
    "\n",
    "figsize = (int(width_ratio * n_cols), int(height_ratio * n_rows))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, image, expected, actual in zip(axes, X_test[:n_total], y_test[:n_total], y_pred[:n_total]):\n",
    "    sns.heatmap(image, vmin=0.0, vmax=1.0, cmap='gray', ax=ax)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    color = 'green' if expected == actual else 'red'\n",
    "    ax.set_title('Predicted %s%d' % ('' if actual == 1 else 'not ', target), color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(X[0], cmap='gray')\n",
    "ax.set(title='Input Image')\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "for i, layer in enumerate(clf.hidden_layers):              \n",
    "    ax = sns.heatmap(layer.pixel_values, vmin=0, vmax=1, cmap='gray')\n",
    "    trained_pixels = clf.grid_W_map[i].keys() \n",
    "\n",
    "    min_extent = clf.layer_shape\n",
    "    max_extent = (0, 0)\n",
    "\n",
    "    for grid_coord in trained_pixels:\n",
    "        grid_row, grid_col = grid_coord\n",
    "\n",
    "        if grid_row < min_extent[0]:\n",
    "            min_extent = (grid_row, min_extent[1])\n",
    "\n",
    "        if grid_row > max_extent[0]:\n",
    "            max_extent = (grid_row, max_extent[1])\n",
    "\n",
    "        if grid_col < min_extent[1]:\n",
    "            min_extent = (min_extent[0], grid_col)\n",
    "\n",
    "        if grid_col > max_extent[1]:\n",
    "            max_extent = (max_extent[0], grid_col)\n",
    "\n",
    "    height = (max_extent[0] - min_extent[0]) + 0.9\n",
    "    width = (max_extent[1] - min_extent[1]) + 0.9\n",
    "\n",
    "    rect = patches.Rectangle((min_extent[1], min_extent[0]), width, height, \n",
    "                             linewidth=3, alpha=0.4, \n",
    "                             edgecolor='r', facecolor='none',\n",
    "                             label='Trained Pixels Area')\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.set(title='Hidden Layer %d Pixel Transparency Values' % i)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "        \n",
    "ax = sns.heatmap(clf.predict_proba(X).detach().numpy()[0].reshape(1, -1), vmin=0, cmap='gray')\n",
    "ax.set(title='Output')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
