{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "from raytracerthing import RayTracerThing, Activations\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "digits.keys()\n",
    "\n",
    "y = digits['target']\n",
    "# y = np.array(y==1, dtype=int)\n",
    "ones = y[digits['target'] == target]\n",
    "not_ones = y[digits['target'] != target]\n",
    "\n",
    "n = min(len(ones), len(not_ones))\n",
    "\n",
    "y = np.concatenate([np.ones(n, dtype=int), np.zeros(n, dtype=int)])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 (8, 8)\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = digits['images']\n",
    "X = np.concatenate([X[digits['target'] == target][:n], X[digits['target'] != target][:n]])\n",
    "\n",
    "X = X / X.max()\n",
    "N = X.shape[0]\n",
    "image_shape = X.shape[1:]\n",
    "\n",
    "print(N, image_shape)\n",
    "\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([178, 178]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.8 ms, sys: 155 Âµs, total: 99 ms\n",
      "Wall time: 97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = RayTracerThing(input_shape=image_shape,\n",
    "                     hidden_layer_shape=image_shape, \n",
    "                     n_layers=3,\n",
    "                     n_classes=2,\n",
    "                     activation_func=lambda x: Activations.sigmoid(x, alpha=10),\n",
    "                     loss_func=torch.nn.functional.binary_cross_entropy,\n",
    "                     learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2000 - train_loss: 0.4406 - train_acc: 0.8500 - val_loss: 0.5121 - val_acc: 0.7368\n",
      "Epoch 2 of 2000 - train_loss: 0.3405 - train_acc: 0.8958 - val_loss: 0.4732 - val_acc: 0.7544\n",
      "Epoch 3 of 2000 - train_loss: 0.3250 - train_acc: 0.9083 - val_loss: 0.4586 - val_acc: 0.7895\n",
      "Epoch 4 of 2000 - train_loss: 0.3173 - train_acc: 0.9125 - val_loss: 0.4485 - val_acc: 0.8070\n",
      "Epoch 5 of 2000 - train_loss: 0.3089 - train_acc: 0.9125 - val_loss: 0.4433 - val_acc: 0.7895\n",
      "Epoch 6 of 2000 - train_loss: 0.3028 - train_acc: 0.9125 - val_loss: 0.4381 - val_acc: 0.8070\n",
      "Epoch 7 of 2000 - train_loss: 0.2969 - train_acc: 0.9167 - val_loss: 0.4310 - val_acc: 0.8070\n",
      "Epoch 8 of 2000 - train_loss: 0.2897 - train_acc: 0.9333 - val_loss: 0.4272 - val_acc: 0.8070\n",
      "Epoch 9 of 2000 - train_loss: 0.2871 - train_acc: 0.9333 - val_loss: 0.4257 - val_acc: 0.8070\n",
      "Epoch 10 of 2000 - train_loss: 0.2859 - train_acc: 0.9375 - val_loss: 0.4247 - val_acc: 0.8246\n",
      "Epoch 11 of 2000 - train_loss: 0.2848 - train_acc: 0.9375 - val_loss: 0.4237 - val_acc: 0.8246\n",
      "Epoch 12 of 2000 - train_loss: 0.2838 - train_acc: 0.9375 - val_loss: 0.4230 - val_acc: 0.8421\n",
      "Epoch 13 of 2000 - train_loss: 0.2829 - train_acc: 0.9375 - val_loss: 0.4226 - val_acc: 0.8421\n",
      "Epoch 14 of 2000 - train_loss: 0.2822 - train_acc: 0.9333 - val_loss: 0.4223 - val_acc: 0.8421\n",
      "Epoch 15 of 2000 - train_loss: 0.2815 - train_acc: 0.9333 - val_loss: 0.4221 - val_acc: 0.8421\n",
      "Epoch 16 of 2000 - train_loss: 0.2809 - train_acc: 0.9333 - val_loss: 0.4218 - val_acc: 0.8421\n",
      "Epoch 17 of 2000 - train_loss: 0.2802 - train_acc: 0.9333 - val_loss: 0.4213 - val_acc: 0.8421\n",
      "Epoch 18 of 2000 - train_loss: 0.2794 - train_acc: 0.9333 - val_loss: 0.4207 - val_acc: 0.8421\n",
      "Epoch 19 of 2000 - train_loss: 0.2788 - train_acc: 0.9333 - val_loss: 0.4203 - val_acc: 0.8421\n",
      "Epoch 20 of 2000 - train_loss: 0.2783 - train_acc: 0.9333 - val_loss: 0.4200 - val_acc: 0.8421\n",
      "Epoch 21 of 2000 - train_loss: 0.2779 - train_acc: 0.9292 - val_loss: 0.4197 - val_acc: 0.8421\n",
      "Epoch 22 of 2000 - train_loss: 0.2775 - train_acc: 0.9292 - val_loss: 0.4195 - val_acc: 0.8421\n",
      "Epoch 23 of 2000 - train_loss: 0.2772 - train_acc: 0.9292 - val_loss: 0.4193 - val_acc: 0.8421\n",
      "Epoch 24 of 2000 - train_loss: 0.2768 - train_acc: 0.9292 - val_loss: 0.4192 - val_acc: 0.8421\n",
      "Epoch 25 of 2000 - train_loss: 0.2764 - train_acc: 0.9292 - val_loss: 0.4187 - val_acc: 0.8421\n",
      "Epoch 26 of 2000 - train_loss: 0.2758 - train_acc: 0.9292 - val_loss: 0.4183 - val_acc: 0.8421\n",
      "Epoch 27 of 2000 - train_loss: 0.2754 - train_acc: 0.9292 - val_loss: 0.4179 - val_acc: 0.8421\n",
      "Epoch 28 of 2000 - train_loss: 0.2749 - train_acc: 0.9292 - val_loss: 0.4176 - val_acc: 0.8421\n",
      "Epoch 29 of 2000 - train_loss: 0.2746 - train_acc: 0.9292 - val_loss: 0.4173 - val_acc: 0.8421\n",
      "Epoch 30 of 2000 - train_loss: 0.2743 - train_acc: 0.9333 - val_loss: 0.4171 - val_acc: 0.8246\n",
      "Epoch 31 of 2000 - train_loss: 0.2740 - train_acc: 0.9333 - val_loss: 0.4169 - val_acc: 0.8246\n",
      "Epoch 32 of 2000 - train_loss: 0.2738 - train_acc: 0.9333 - val_loss: 0.4168 - val_acc: 0.8246\n",
      "Epoch 33 of 2000 - train_loss: 0.2735 - train_acc: 0.9333 - val_loss: 0.4166 - val_acc: 0.8246\n",
      "Epoch 34 of 2000 - train_loss: 0.2732 - train_acc: 0.9333 - val_loss: 0.4164 - val_acc: 0.8246\n",
      "Epoch 35 of 2000 - train_loss: 0.2729 - train_acc: 0.9333 - val_loss: 0.4161 - val_acc: 0.8246\n",
      "Epoch 36 of 2000 - train_loss: 0.2724 - train_acc: 0.9333 - val_loss: 0.4154 - val_acc: 0.8246\n",
      "Epoch 37 of 2000 - train_loss: 0.2717 - train_acc: 0.9333 - val_loss: 0.4141 - val_acc: 0.8246\n",
      "Epoch 38 of 2000 - train_loss: 0.2705 - train_acc: 0.9292 - val_loss: 0.4110 - val_acc: 0.8246\n",
      "Epoch 39 of 2000 - train_loss: 0.2688 - train_acc: 0.9292 - val_loss: 0.4059 - val_acc: 0.8246\n",
      "Epoch 40 of 2000 - train_loss: 0.2668 - train_acc: 0.9250 - val_loss: 0.4023 - val_acc: 0.8246\n",
      "Epoch 41 of 2000 - train_loss: 0.2638 - train_acc: 0.9375 - val_loss: 0.4006 - val_acc: 0.8246\n",
      "Epoch 42 of 2000 - train_loss: 0.2622 - train_acc: 0.9375 - val_loss: 0.3995 - val_acc: 0.8421\n",
      "Epoch 43 of 2000 - train_loss: 0.2612 - train_acc: 0.9375 - val_loss: 0.3987 - val_acc: 0.8421\n",
      "Epoch 44 of 2000 - train_loss: 0.2604 - train_acc: 0.9375 - val_loss: 0.3981 - val_acc: 0.8421\n",
      "Epoch 45 of 2000 - train_loss: 0.2598 - train_acc: 0.9375 - val_loss: 0.3976 - val_acc: 0.8246\n",
      "Epoch 46 of 2000 - train_loss: 0.2593 - train_acc: 0.9375 - val_loss: 0.3971 - val_acc: 0.8246\n",
      "Epoch 47 of 2000 - train_loss: 0.2588 - train_acc: 0.9375 - val_loss: 0.3967 - val_acc: 0.8246\n",
      "Epoch 48 of 2000 - train_loss: 0.2584 - train_acc: 0.9375 - val_loss: 0.3964 - val_acc: 0.8246\n",
      "Epoch 49 of 2000 - train_loss: 0.2581 - train_acc: 0.9375 - val_loss: 0.3962 - val_acc: 0.8246\n",
      "Epoch 50 of 2000 - train_loss: 0.2578 - train_acc: 0.9375 - val_loss: 0.3959 - val_acc: 0.8246\n",
      "Epoch 51 of 2000 - train_loss: 0.2575 - train_acc: 0.9375 - val_loss: 0.3957 - val_acc: 0.8246\n",
      "Epoch 52 of 2000 - train_loss: 0.2572 - train_acc: 0.9375 - val_loss: 0.3956 - val_acc: 0.8246\n",
      "Epoch 53 of 2000 - train_loss: 0.2569 - train_acc: 0.9375 - val_loss: 0.3953 - val_acc: 0.8246\n",
      "Epoch 54 of 2000 - train_loss: 0.2566 - train_acc: 0.9375 - val_loss: 0.3951 - val_acc: 0.8246\n",
      "Epoch 55 of 2000 - train_loss: 0.2564 - train_acc: 0.9375 - val_loss: 0.3950 - val_acc: 0.8246\n",
      "Epoch 56 of 2000 - train_loss: 0.2562 - train_acc: 0.9375 - val_loss: 0.3948 - val_acc: 0.8246\n",
      "Epoch 57 of 2000 - train_loss: 0.2560 - train_acc: 0.9375 - val_loss: 0.3947 - val_acc: 0.8246\n",
      "Epoch 58 of 2000 - train_loss: 0.2558 - train_acc: 0.9375 - val_loss: 0.3945 - val_acc: 0.8246\n",
      "Epoch 59 of 2000 - train_loss: 0.2556 - train_acc: 0.9375 - val_loss: 0.3944 - val_acc: 0.8246\n",
      "Epoch 60 of 2000 - train_loss: 0.2555 - train_acc: 0.9375 - val_loss: 0.3942 - val_acc: 0.8246\n",
      "Epoch 61 of 2000 - train_loss: 0.2553 - train_acc: 0.9375 - val_loss: 0.3941 - val_acc: 0.8246\n",
      "Epoch 62 of 2000 - train_loss: 0.2552 - train_acc: 0.9375 - val_loss: 0.3940 - val_acc: 0.8246\n",
      "Epoch 63 of 2000 - train_loss: 0.2550 - train_acc: 0.9375 - val_loss: 0.3939 - val_acc: 0.8246\n",
      "Epoch 64 of 2000 - train_loss: 0.2549 - train_acc: 0.9333 - val_loss: 0.3938 - val_acc: 0.8246\n",
      "Epoch 65 of 2000 - train_loss: 0.2548 - train_acc: 0.9333 - val_loss: 0.3936 - val_acc: 0.8246\n",
      "Epoch 66 of 2000 - train_loss: 0.2546 - train_acc: 0.9333 - val_loss: 0.3935 - val_acc: 0.8246\n",
      "Epoch 67 of 2000 - train_loss: 0.2545 - train_acc: 0.9333 - val_loss: 0.3934 - val_acc: 0.8246\n",
      "Epoch 68 of 2000 - train_loss: 0.2544 - train_acc: 0.9333 - val_loss: 0.3932 - val_acc: 0.8246\n",
      "Epoch 69 of 2000 - train_loss: 0.2542 - train_acc: 0.9333 - val_loss: 0.3931 - val_acc: 0.8246\n",
      "Epoch 70 of 2000 - train_loss: 0.2541 - train_acc: 0.9333 - val_loss: 0.3929 - val_acc: 0.8246\n",
      "Epoch 71 of 2000 - train_loss: 0.2539 - train_acc: 0.9333 - val_loss: 0.3927 - val_acc: 0.8246\n",
      "Epoch 72 of 2000 - train_loss: 0.2536 - train_acc: 0.9333 - val_loss: 0.3924 - val_acc: 0.8246\n",
      "Epoch 73 of 2000 - train_loss: 0.2534 - train_acc: 0.9333 - val_loss: 0.3921 - val_acc: 0.8246\n",
      "Epoch 74 of 2000 - train_loss: 0.2532 - train_acc: 0.9333 - val_loss: 0.3917 - val_acc: 0.8246\n",
      "Epoch 75 of 2000 - train_loss: 0.2530 - train_acc: 0.9333 - val_loss: 0.3913 - val_acc: 0.8246\n",
      "Epoch 76 of 2000 - train_loss: 0.2527 - train_acc: 0.9333 - val_loss: 0.3907 - val_acc: 0.8246\n",
      "Epoch 77 of 2000 - train_loss: 0.2525 - train_acc: 0.9333 - val_loss: 0.3901 - val_acc: 0.8246\n",
      "Epoch 78 of 2000 - train_loss: 0.2522 - train_acc: 0.9333 - val_loss: 0.3895 - val_acc: 0.8246\n",
      "Epoch 79 of 2000 - train_loss: 0.2519 - train_acc: 0.9333 - val_loss: 0.3887 - val_acc: 0.8246\n",
      "Epoch 80 of 2000 - train_loss: 0.2517 - train_acc: 0.9333 - val_loss: 0.3879 - val_acc: 0.8246\n",
      "Epoch 81 of 2000 - train_loss: 0.2513 - train_acc: 0.9333 - val_loss: 0.3873 - val_acc: 0.8421\n",
      "Epoch 82 of 2000 - train_loss: 0.2511 - train_acc: 0.9333 - val_loss: 0.3868 - val_acc: 0.8421\n",
      "Epoch 83 of 2000 - train_loss: 0.2509 - train_acc: 0.9333 - val_loss: 0.3863 - val_acc: 0.8421\n",
      "Epoch 84 of 2000 - train_loss: 0.2506 - train_acc: 0.9333 - val_loss: 0.3859 - val_acc: 0.8421\n",
      "Epoch 85 of 2000 - train_loss: 0.2503 - train_acc: 0.9333 - val_loss: 0.3858 - val_acc: 0.8421\n",
      "Epoch 86 of 2000 - train_loss: 0.2502 - train_acc: 0.9333 - val_loss: 0.3857 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 of 2000 - train_loss: 0.2501 - train_acc: 0.9333 - val_loss: 0.3857 - val_acc: 0.8421\n",
      "Epoch 88 of 2000 - train_loss: 0.2499 - train_acc: 0.9333 - val_loss: 0.3856 - val_acc: 0.8421\n",
      "Epoch 89 of 2000 - train_loss: 0.2499 - train_acc: 0.9333 - val_loss: 0.3856 - val_acc: 0.8421\n",
      "Epoch 90 of 2000 - train_loss: 0.2498 - train_acc: 0.9333 - val_loss: 0.3855 - val_acc: 0.8421\n",
      "Epoch 91 of 2000 - train_loss: 0.2497 - train_acc: 0.9333 - val_loss: 0.3855 - val_acc: 0.8421\n",
      "Epoch 92 of 2000 - train_loss: 0.2497 - train_acc: 0.9333 - val_loss: 0.3855 - val_acc: 0.8421\n",
      "Epoch 93 of 2000 - train_loss: 0.2496 - train_acc: 0.9333 - val_loss: 0.3854 - val_acc: 0.8421\n",
      "Epoch 94 of 2000 - train_loss: 0.2496 - train_acc: 0.9333 - val_loss: 0.3854 - val_acc: 0.8421\n",
      "Epoch 95 of 2000 - train_loss: 0.2496 - train_acc: 0.9333 - val_loss: 0.3854 - val_acc: 0.8421\n",
      "Epoch 96 of 2000 - train_loss: 0.2495 - train_acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8421\n",
      "Epoch 97 of 2000 - train_loss: 0.2495 - train_acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8421\n",
      "Epoch 98 of 2000 - train_loss: 0.2495 - train_acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8421\n",
      "Epoch 99 of 2000 - train_loss: 0.2495 - train_acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8421\n",
      "Epoch 100 of 2000 - train_loss: 0.2495 - train_acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8421\n",
      "Epoch 101 of 2000 - train_loss: 0.2494 - train_acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8421\n",
      "Epoch 102 of 2000 - train_loss: 0.2494 - train_acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8421\n",
      "Epoch 103 of 2000 - train_loss: 0.2494 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 104 of 2000 - train_loss: 0.2494 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 105 of 2000 - train_loss: 0.2493 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 106 of 2000 - train_loss: 0.2493 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 107 of 2000 - train_loss: 0.2493 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 108 of 2000 - train_loss: 0.2493 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 109 of 2000 - train_loss: 0.2493 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 110 of 2000 - train_loss: 0.2493 - train_acc: 0.9333 - val_loss: 0.3852 - val_acc: 0.8421\n",
      "Epoch 111 of 2000 - train_loss: 0.2493 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 112 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 113 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 114 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 115 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 116 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 117 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 118 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 119 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 120 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 121 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 122 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 123 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 124 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 125 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 126 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 127 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 128 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 129 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3851 - val_acc: 0.8421\n",
      "Epoch 130 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 131 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 132 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 133 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 134 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 135 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 136 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 137 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 138 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 139 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 140 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 141 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 142 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 143 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 144 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 145 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 146 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 147 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 148 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 149 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 150 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 151 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 152 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 153 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 154 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 155 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 156 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 157 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 158 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 159 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 160 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 161 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 162 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 163 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 164 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 165 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 166 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 167 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 168 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 169 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 170 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 171 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 173 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 174 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 175 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 176 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 177 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 178 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 179 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 180 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 181 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 182 of 2000 - train_loss: 0.2492 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 183 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 184 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 185 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 186 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 187 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 188 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3850 - val_acc: 0.8421\n",
      "Epoch 189 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 190 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 191 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 192 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 193 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 194 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 195 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 196 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 197 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 198 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 199 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 200 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 201 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 202 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 203 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 204 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 205 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 206 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 207 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 208 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 209 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 210 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 211 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 212 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 213 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 214 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 215 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 216 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 217 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 218 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 219 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 220 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 221 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 222 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 223 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 224 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 225 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 226 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 227 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 228 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 229 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 230 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 231 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 232 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 233 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 234 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 235 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 236 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 237 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 238 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 239 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 240 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 241 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 242 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 243 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 244 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 245 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 246 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 247 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 248 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 249 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 250 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 251 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 252 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 253 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 254 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 255 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 256 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 258 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 259 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 260 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 261 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 262 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 263 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 264 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 265 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 266 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 267 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 268 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 269 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 270 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 271 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 272 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 273 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 274 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 275 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 276 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 277 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 278 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 279 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 280 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 281 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 282 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 283 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 284 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 285 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 286 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 287 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 288 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 289 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 290 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 291 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 292 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 293 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 294 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 295 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 296 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 297 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 298 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 299 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 300 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 301 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3849 - val_acc: 0.8421\n",
      "Epoch 302 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 303 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 304 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 305 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 306 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 307 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 308 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 309 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 310 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 311 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 312 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 313 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 314 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 315 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 316 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 317 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 318 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 319 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 320 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 321 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 322 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 323 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 324 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 325 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 326 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 327 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 328 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 329 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 330 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 331 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 332 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 333 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 334 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 335 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 336 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 337 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 338 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 339 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 340 of 2000 - train_loss: 0.2491 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 341 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 343 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 344 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 345 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 346 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 347 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 348 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 349 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 350 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 351 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 352 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 353 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 354 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 355 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 356 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 357 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 358 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 359 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 360 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 361 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 362 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 363 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 364 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 365 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 366 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 367 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 368 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 369 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 370 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 371 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 372 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 373 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 374 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 375 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 376 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 377 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 378 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 379 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 380 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 381 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 382 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 383 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 384 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 385 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 386 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 387 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 388 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 389 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 390 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 391 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 392 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 393 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 394 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 395 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 396 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 397 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 398 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 399 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 400 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 401 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 402 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 403 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 404 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 405 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 406 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 407 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 408 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 409 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 410 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 411 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 412 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 413 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 414 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 415 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 416 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 417 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 418 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 419 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 420 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 421 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 422 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 423 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 424 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 425 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 426 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 428 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 429 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 430 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 431 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 432 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 433 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 434 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 435 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 436 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 437 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 438 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 439 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 440 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 441 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 442 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 443 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 444 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 445 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 446 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 447 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 448 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 449 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 450 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 451 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 452 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 453 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 454 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 455 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 456 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 457 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 458 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 459 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 460 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 461 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 462 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 463 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 464 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 465 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 466 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 467 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 468 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 469 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 470 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 471 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 472 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 473 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 474 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 475 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 476 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 477 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 478 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 479 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 480 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 481 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 482 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 483 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 484 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 485 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 486 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 487 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 488 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 489 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 490 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 491 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 492 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 493 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 494 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 495 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 496 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 497 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 498 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 499 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 500 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 501 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 502 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 503 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 504 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 505 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 506 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 507 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 508 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 509 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 510 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 511 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 512 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 513 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 514 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 515 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 516 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 517 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 518 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 519 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 520 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 521 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 522 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 523 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 524 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 525 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 526 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 527 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 528 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 529 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 530 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 531 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 532 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 533 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 534 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 535 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 536 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 537 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 538 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 539 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 540 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 541 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 542 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 543 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 544 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 545 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 546 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 547 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 548 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 549 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 550 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 551 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 552 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 553 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 554 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 555 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 556 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 557 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 558 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 559 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 560 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 561 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 562 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 563 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 564 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 565 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 566 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 567 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 568 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 569 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 570 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 571 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 572 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 573 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 574 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 575 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 576 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 577 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 578 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 579 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 580 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 581 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 582 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 583 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 584 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 585 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 586 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 587 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 588 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 589 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 590 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 591 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 592 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 593 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 594 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 595 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 596 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 597 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 598 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 599 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 600 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 601 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 602 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 603 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 604 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 605 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 606 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 607 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 608 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 609 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 610 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 611 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 612 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 613 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 614 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 615 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 616 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 617 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 618 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 619 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 620 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 621 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 622 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 623 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 624 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 625 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 626 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 627 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 628 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 629 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 630 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 631 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 632 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 633 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 634 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 635 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 636 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 637 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 638 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 639 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 640 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 641 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 642 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 643 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 644 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 645 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 646 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 647 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 648 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 649 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3848 - val_acc: 0.8421\n",
      "Epoch 650 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 651 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 652 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 653 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 654 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 655 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 656 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 657 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 658 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 659 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 660 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 661 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 662 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 663 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 664 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 665 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 666 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 667 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 668 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 669 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 670 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 671 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 672 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 673 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 674 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 675 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 676 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 677 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 678 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 679 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 680 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 681 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 682 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 683 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 684 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 685 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 686 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 687 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 688 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 689 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 690 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 691 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 692 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 693 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 694 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 695 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 696 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 697 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 698 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 699 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 700 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 701 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 702 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 703 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 704 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 705 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 706 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 707 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 708 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 709 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 710 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 711 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 712 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 713 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 714 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 715 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 716 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 717 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 718 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 719 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 720 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 721 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 722 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 723 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 724 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 725 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 726 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 727 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 728 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 729 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 730 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 731 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 732 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 733 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 734 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 735 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 736 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 737 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 738 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 739 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 740 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 741 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 742 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 743 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 744 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 745 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 746 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 747 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 748 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 749 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 750 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 751 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 752 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 753 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 754 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 755 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 756 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 757 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 758 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 759 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 760 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 761 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 762 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 763 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 764 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 765 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 766 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 768 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 769 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 770 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 771 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 772 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 773 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 774 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 775 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 776 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 777 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 778 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 779 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 780 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 781 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 782 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 783 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 784 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 785 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 786 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 787 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 788 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 789 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 790 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 791 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 792 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 793 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 794 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 795 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 796 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 797 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 798 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 799 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 800 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 801 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 802 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 803 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 804 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 805 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 806 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 807 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 808 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 809 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 810 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 811 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 812 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 813 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 814 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 815 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 816 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 817 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 818 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 819 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 820 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 821 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 822 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 823 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 824 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 825 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 826 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 827 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 828 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 829 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 830 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 831 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 832 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 833 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 834 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 835 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 836 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 837 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 838 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 839 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 840 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 841 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 842 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 843 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 844 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 845 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 846 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 847 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 848 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 849 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 850 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 851 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 852 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 853 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 854 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 855 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 856 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 857 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 858 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 859 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 860 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 861 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 862 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 863 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 864 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 865 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 866 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 867 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 868 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 869 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 870 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 871 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 872 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 873 of 2000 - train_loss: 0.2490 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 874 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 875 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 876 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 877 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 878 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 879 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 880 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 881 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 882 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 883 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 884 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 885 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 886 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 887 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 888 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 889 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 890 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 891 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 892 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 893 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 894 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 895 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 896 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 897 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 898 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 899 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 900 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 901 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 902 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 903 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 904 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 905 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 906 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 907 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 908 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 909 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 910 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 911 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 912 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 913 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 914 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 915 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 916 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 917 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 918 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 919 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 920 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 921 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 922 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 923 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 924 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 925 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 926 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 927 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 928 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 929 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 930 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 931 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 932 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 933 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 934 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 935 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 936 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 937 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 938 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 939 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 940 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 941 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 942 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 943 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 944 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 945 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 946 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 947 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 948 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 949 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 950 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 951 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 952 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 953 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 954 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 955 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 956 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 957 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 958 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 959 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 960 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 961 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 962 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 963 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 964 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 965 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 966 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 967 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 968 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 969 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 970 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 971 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 972 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 973 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 974 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 975 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 976 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 977 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 978 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 979 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 980 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 981 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 982 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 983 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 984 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 985 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 986 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 987 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 988 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 989 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 990 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 991 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 992 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 993 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 994 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 995 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 996 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 997 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 998 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 999 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1000 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1001 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1002 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1003 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1004 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1005 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1006 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1007 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1008 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1009 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1010 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1011 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1012 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1013 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1014 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1015 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1016 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1017 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1018 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1019 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1020 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1021 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1022 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1023 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1024 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1025 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1026 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1027 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1028 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1029 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1030 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1031 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1032 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1033 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1034 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1035 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1036 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1037 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1038 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1039 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1040 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1041 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1042 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1043 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1044 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1045 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1046 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1047 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1048 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1049 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1050 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1051 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1052 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1053 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1054 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1055 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1056 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1057 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1058 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1059 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1060 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1061 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1062 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1063 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1064 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1065 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1066 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1067 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1068 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1069 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1070 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1071 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1072 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1073 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1074 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1075 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1076 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1077 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1078 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1079 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1080 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1081 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1082 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1083 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1084 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1085 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1086 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1087 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1088 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1089 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1090 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1091 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1092 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1093 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1094 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1095 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1096 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1097 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1098 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1099 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1100 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1101 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1102 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1103 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1104 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1105 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1106 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1107 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1108 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1109 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1110 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1111 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1112 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1113 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1114 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1115 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1116 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1117 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1118 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1119 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1120 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1121 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1122 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1123 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1124 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1125 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1126 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1127 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1128 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1129 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1130 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1131 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1132 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1133 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1134 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1135 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1136 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1137 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1138 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1139 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1140 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1141 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1142 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1143 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1144 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1145 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1146 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1147 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1148 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1149 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1150 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1151 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1152 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1153 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1154 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1155 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1156 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1157 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1158 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1159 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1160 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1161 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1162 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1163 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1164 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1165 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1166 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1167 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1168 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1169 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1170 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1171 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1172 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1173 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1174 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1175 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1176 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1177 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1178 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1179 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1180 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1181 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1182 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1183 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1184 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1185 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1186 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1187 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1188 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1189 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1190 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1191 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1192 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1193 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1194 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1195 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1196 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1197 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1198 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1199 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1200 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1201 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1202 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1203 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1204 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1205 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1206 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1207 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1208 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1209 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1210 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1211 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1212 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1213 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1214 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1215 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1216 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1217 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1218 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1219 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1220 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1221 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1222 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1223 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1224 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1225 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1226 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1227 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1228 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1229 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1230 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1231 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1232 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1233 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1234 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1235 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1236 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1237 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1238 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1239 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1240 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1241 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1242 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1243 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1244 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1245 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1246 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1247 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1248 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1249 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1250 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1251 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1252 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1253 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1254 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1255 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1256 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1257 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1258 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1259 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1260 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1261 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1262 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1263 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1264 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1265 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1266 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1267 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1268 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1269 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1270 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1271 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1272 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1273 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1274 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1275 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1276 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1277 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1278 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1279 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1280 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1281 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1282 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1283 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1284 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1285 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1286 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1287 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1288 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1289 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1290 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1291 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1292 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1293 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1294 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1295 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1296 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1297 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1298 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1299 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1300 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1301 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1302 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1303 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1304 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1305 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1306 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1307 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1308 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1309 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1310 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1311 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1312 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1313 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1314 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1315 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1316 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1317 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1318 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1319 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1320 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1321 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1322 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1323 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1324 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1325 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1326 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1327 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1328 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1329 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1330 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1331 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1332 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1333 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1334 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1335 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1336 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1337 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1338 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1339 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1340 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1341 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1342 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1343 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1344 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1345 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1346 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1347 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1348 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1349 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1350 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1351 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1352 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1353 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1354 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1355 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1356 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1357 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1358 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1359 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1360 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1361 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1362 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1363 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1364 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1365 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1366 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1367 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1368 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1369 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1370 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1371 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1372 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1373 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1374 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1375 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1376 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1377 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1378 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1379 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1380 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1381 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1382 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1383 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1384 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1385 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1386 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1387 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1388 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1389 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1390 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1391 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1392 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1393 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1394 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1395 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1396 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1397 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1398 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1399 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1400 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1401 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1402 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1403 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1404 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1405 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1406 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1407 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1408 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1409 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1410 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1411 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1412 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1413 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1414 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1415 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1416 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1417 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1418 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1419 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1420 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1421 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1422 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1423 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1424 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1425 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1426 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1427 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1428 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1429 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1430 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1431 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1432 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1433 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1434 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1435 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1436 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1437 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1438 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1439 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1440 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1441 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1442 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1443 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1444 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1445 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1446 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1447 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1448 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1449 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1450 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1451 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1452 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1453 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1454 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1455 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1456 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1457 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1458 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1459 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1460 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1461 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1462 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1463 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1464 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1465 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1466 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1467 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1468 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1469 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1470 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1471 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1472 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1473 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1474 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1475 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1476 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1477 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1478 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1479 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1480 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1481 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1482 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1483 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1484 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1485 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1486 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1487 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1488 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1489 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1490 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1491 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1492 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1493 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1494 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1495 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1496 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1497 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1498 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1499 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1500 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1501 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1502 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1503 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1504 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1505 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1506 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1507 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1508 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1509 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1510 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1511 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1512 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1513 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1514 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1515 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1516 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1517 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1518 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1519 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1520 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1521 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1522 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1523 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1524 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1525 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1526 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1527 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1528 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1529 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1530 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1531 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1532 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1533 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1534 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1535 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1536 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1537 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1538 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1539 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1540 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1541 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1542 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1543 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1544 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1545 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1546 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1547 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1548 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1549 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1550 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1551 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1552 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1553 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1554 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1555 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1556 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1557 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1558 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1559 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1560 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1561 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1562 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1563 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1564 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1565 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1566 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1567 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1568 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1569 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1570 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1571 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1572 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1573 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1574 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1575 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1576 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1577 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1578 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1579 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1580 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1581 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1582 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1583 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1584 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1585 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1586 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1587 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1588 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1589 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1590 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1591 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1592 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1593 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1594 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1595 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1596 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1597 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1598 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1599 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1600 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1601 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1602 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1603 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1604 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1605 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1606 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1607 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1608 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1609 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1610 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1611 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1612 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1613 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1614 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1615 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1616 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1617 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1618 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1619 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1620 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1621 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1622 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1623 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1624 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1625 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1626 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1627 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1628 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1629 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1630 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1631 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1632 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1633 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1634 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1635 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1636 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1637 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1638 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1639 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1640 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1641 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1642 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1643 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1644 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1645 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1646 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1647 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1648 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1649 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1650 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1651 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1652 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1653 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1654 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1655 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1656 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1657 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1658 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1659 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1660 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1661 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1662 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1663 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1664 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1665 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1666 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1667 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1668 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1669 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1670 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1671 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1672 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1673 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1674 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1675 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1676 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1677 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1678 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1679 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1680 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1681 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1682 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1683 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1684 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1685 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1686 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1687 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1688 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1689 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1690 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1691 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1692 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1693 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1694 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1695 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1696 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1697 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1698 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1699 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1700 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1701 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1702 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1703 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1704 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1705 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1706 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1707 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1708 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1709 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1710 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1711 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1712 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1713 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1714 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1715 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1716 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1717 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1718 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1719 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1720 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1721 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1722 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1723 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1724 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1725 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1726 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1727 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1728 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1729 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1730 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1731 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1732 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1733 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1734 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1735 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1736 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1737 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1738 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1739 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1740 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1741 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1742 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1743 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1744 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1745 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1746 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1747 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1748 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1749 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1750 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1751 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1752 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1753 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1754 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1755 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1756 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1757 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1758 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1759 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1760 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1761 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1762 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1763 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1764 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1765 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1766 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1767 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1768 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1769 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1770 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1771 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1772 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1773 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1774 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1775 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1776 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1777 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1778 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1779 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1780 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1781 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1782 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1783 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1784 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1785 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1786 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1787 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1788 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1789 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1790 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1791 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1792 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1793 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1794 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1795 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1796 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1797 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1798 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1799 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1800 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1801 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1802 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1803 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1804 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1805 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1806 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1807 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1808 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1809 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1810 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1811 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1812 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1813 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1814 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1815 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1816 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1817 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1818 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1819 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1820 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1821 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1822 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1823 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1824 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1825 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1826 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1827 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1828 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1829 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1830 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1831 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1832 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1833 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1834 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1835 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1836 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1837 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1838 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1839 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1840 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1841 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1842 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1843 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1844 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1845 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1846 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1847 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1848 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1849 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1850 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1851 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1852 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1853 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1854 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1855 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1856 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1857 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1858 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1859 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1860 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1861 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1862 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1863 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1864 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1865 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1866 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1867 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1868 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1869 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1870 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1871 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1872 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1873 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1874 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1875 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1876 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1877 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1878 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1879 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1880 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1881 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1882 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1883 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1884 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1885 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1886 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1887 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1888 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1889 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1890 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1891 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1892 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1893 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1894 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1895 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1896 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1897 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1898 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1899 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1900 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1901 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1902 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1903 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1904 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1905 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1906 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1907 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1908 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1909 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1910 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1911 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1912 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1913 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1914 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1915 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1916 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1917 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1918 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1919 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1920 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1921 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1922 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1923 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1924 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1925 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1926 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1927 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1928 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1929 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1930 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1931 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1932 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1933 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1934 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1935 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1936 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1937 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1938 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1939 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1940 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1941 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1942 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1943 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1944 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1945 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1946 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1947 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1948 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1949 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1950 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1951 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1952 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1953 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1954 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1955 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1956 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1957 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1958 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1959 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1960 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1961 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1962 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1963 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1964 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1965 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1966 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1967 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1968 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1969 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1970 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1971 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1972 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1973 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1974 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1975 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1976 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1977 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1978 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1979 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1980 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1981 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1982 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1983 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1984 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1985 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1986 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1987 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1988 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1989 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1990 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1991 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1992 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1993 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1994 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1995 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1996 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1997 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1998 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 1999 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "Epoch 2000 of 2000 - train_loss: 0.2489 - train_acc: 0.9333 - val_loss: 0.3847 - val_acc: 0.8421\n",
      "\n",
      "CPU times: user 11min 45s, sys: 5.14 s, total: 11min 50s\n",
      "Wall time: 11min 48s\n"
     ]
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train, batch_size=16, n_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9167)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92        36\n",
      "           1       0.97      0.86      0.91        36\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        72\n",
      "   macro avg       0.92      0.92      0.92        72\n",
      "weighted avg       0.92      0.92      0.92        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues')\n",
    "\n",
    "cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "cm.xaxis.tick_top()\n",
    "cm.xaxis.set_label_position('top')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 72 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 6\n",
    "n_cols = 6\n",
    "n_total = n_rows * n_cols\n",
    "n_total = min(n_total, N)\n",
    "\n",
    "width_ratio = 2.7\n",
    "height_ratio = 2\n",
    "\n",
    "figsize = (int(width_ratio * n_cols), int(height_ratio * n_rows))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, image, expected, actual in zip(axes, X_test[:n_total], y_test[:n_total], y_pred[:n_total]):\n",
    "    sns.heatmap(image, vmin=0.0, vmax=1.0, cmap='gray', ax=ax)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    color = 'green' if expected == actual else 'red'\n",
    "    ax.set_title('Predicted %s%d' % ('' if actual == 1 else 'not ', target), color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEICAYAAAD8yyfzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAH8VJREFUeJzt3XuUFOWB/vHvw3BHvE7QFTRBwAuKiTeUGAOKo4JRNm6iQBTRKOZnMB6jEmIkEsNGRTyGRBOE1QAq0USjTowGNMFEXYiQxLhqlpNZQBjxwiggF3GY4f39UQXW9ExfBnqKsXk+5/Q51f1WvfV2VffTb79VXa0QAmZmlo42u7oBZma7E4eumVmKHLpmZily6JqZpciha2aWIoeumVmKPhGhK+k1SYOylA2SVJ1j2ZmSJrVY40qUpFMkLSlCPaMlvVCMNll6JFVne8/ZztnloStpuaTTMx5r8EYNIRwZQngu9cbl0FrDRNJYSYslfSRpZp55R0uql7RB0geSXpb0JYAQwvMhhMNasJ2nxOvdIGmjpJC4v0HSwS217lInaYKkPzbx+P6Stkg6fFe0yyK7PHRtxyjS1P5bBUwC7iuwqgUhhD2AvYF7gV9J2rdIzcwqDvU94nUfGT+897bHQggrkvNLapPl+bYqktru6jYAs4EvNvHBNQL4Wwjhf3dBmyzW6l/E0LA3LKlTPGSwRtLrwAkZ8x4j6W+S1kt6GOiYUf6luEe3VtJ/Szo6Yz3XSXpF0jpJD0tqsHyB7b1E0j/jNiyVdEWi7FVJ5yTut5NUI+lz8f2T4natlfSP5Fc8Sc9J+k9JLwKbgEMy1x1C+E0I4XHgvea0OYSwlSioOwGHJIdtJPWS9L6kY+P7B8ZtHhTf30vSvZLekvSmpEmSypqz/qZIekHSDyUtADYCB0u6LLFt/0/SZYn5T4/34ThJqyWtkjQqUf6lxLLVkq7JWO77kt6TtEzS8MRy58avmfWSVkiakCjrHffSL5G0ApgXP36ypIXxfnxZ0hczntcP4v28XtLvkx90kr4YL7tO0kpJF0kaED+fNon5LpC0OHO7hRDeAP4MXJhRNAqYFS/bR9L8+PnWSLpf0l5Z9sMDkiZmbufE/R6SHou3+TJJ30yUnRS/Hz+Q9I6k25tax24lhLBLb8By4PSMx0YDLzQ1D3Ar8DywL3AQ8CpQHZe1B94ArgHaAV8BtgCT4vJjgXeBE4Ey4OK47g6J9bwEHBjX/0/gG1na3aCNGWVnA70AAQOJAvLYuGwc8HBi3mHA/8TT3YnCcijRB2JFfP9TcflzwAqinmFboF2O7ToJmJln229/DnF9VwPrgb2AQdu2a1x+ebw9OgNzgSmJsseBe4AuQLd4G16Rbzsllv8MEIC2GY+/EO+TI+L92RY4h+jDRsBpwIfA0fH8pwN1wE3x/OcShfWecflq4PPx9L6JfbJtuduBDnG9m4DecflpwFHxPvksUAN8KS7rHbf9F/G26UT0unwPODNe5qx4mf0Sz+tfQJ94mef5+DXaM94H58fPtxz4XFy2BKhIbJ/fAldn2aYXA/+buH8k8BGwb3z/UGAw0XumG/Bixj6tBgbF0w8AExNlpwPL4+ky4GXghriu3vE+GxyXLwJGxNNdgRN3debs6tuub0C0gzYAaxO3TWQP3aXAWYmyMXwcul8k+nqtRPl/J17QPwd+mLH+JcDAxHouTJRNBqZlafdo8oRJYt7Ht705iAJ9PR8HwSPAuHj6O8D9GcvOBS6Op58Dbi5wnYWGbl28zWuAhYntPIhE6MaPVQL/A7zCxx9U+8dv5k6J+UYA8wvdTuQO3e/nWfZJ4Jvx9Onxa6ksUf4+cHw8vQq4DOiaUcfpQC3QOfHYb4DvZlnnXcDt8fS20D04Uf494BcZy/wB+FrieY1PlH0LeDKengD8Ost6vwfMiqfLid4n3bLMu0e8LfrH928DHs2xHb8CLErcLzR0TwaWZtQ1AZiReP99n/gDx7fQaoYX/j2EsPe2G3BljnkPBFYm7r+RUfZmiPd2E+WfBq6Nv/KtlbSWqFdyYGKetxPTm4hevM0iaUj89fD9eB1Did4khBBWEfUq/kPS3sAQ4MFE+76a0b4vAP+WqD753IthYbzdy0MIJ4UQns0x7wyiHt9PQwgfJdrcDngr0eZ7iHpPxdDg+cZDBH9JbNsziLdtrCaEUJ+4n9yHXybq/a6Ih2pOTMz3XghhU+L+G8Svi/ir/XPx1+d1RMGdXGdmOz8NjMjYjydR2OvsIOD/mtgOAPcD/y6pMzCc6IPt3aZmDCFsAB4FRsVDEiOJhxbi53SApF/Fw0EfADObeE6F+DTRsE/yuY4DDojLLwH6AkskvSRp6A6so6S0ltBtjreIXpjbHJxR1l2SspSvBP4zGfAhhM4hhF8Wq3GSOhC92KcA+8cfIk8RfR3eZhbReNtXiQ5kvZlo3/0Z7esSQrg1sewuuSycpD2AHxMdbJuYGINcSdTTLU+0ec8QwpHZ6mqm7c9XUieibwa38PG2nUfDbZu9ohD+EkI4l+gD4UngoUTxfnH92xxM1DMmnu9R4KAQwl7Af2WuM+ODfiVRTzdzPxYynrmSaGiqqfavABYTDUldRBTCucwiCucziY5tPJ0ou41ov/ULIexJ9I0k23bcSDQMss0BiemVwL8ynmvXEMI5cZuXhBCGE23zO4BHtQPHSUrJJzF0fwV8V9I+knoAVyXKFhB9Xf6WpLaSzgP6J8pnAN+QdKIiXSSdLanrDrZFkjomb0TjWh2Ixg/rJA0h6o0lPU40vnw10ZHmbR4AzpF0pqSyuM5B8fMstEFt43aUAdvqKMYR9anAX0MIlwG/A6YBhBDeIgq+OyTtqegsg16SBhZhnZk6EG3f1UC9otPbBheyoKIDsCMl7RlC2EI0xJPsEbch+jBpr+gA4RCigIdoLPL9EMJmSScRBVku9wNfllSR2I+nSjowz3IQvQbOkvQf8b4sl/TZRPls4LvA4cATeeqaTxSYPwfmxM97m65x2TpJBwHX5ajnZeDs+D33b0TDIdssAGolXRs/zzJJ/SQdBxAfBCwP0YHadUQfolvztLukfRJD9wdEX/2WEb3Zt3/ahxBqgfOIPrXXABcQjc1tK19MdEDorri8Kp53R32e6EBO5u1bRB8Oa4i+1lUmFwohfEjUc+qZ0b6VRL2YG4iCZSVwPc3bTzfGbRhP1Jv+MH5sh0kaRnQw6BvxQ98GjpX0tfj+KKIwfJ3oOT9CwyGRogghrCU6SPoY0VjtV4h6rIW6GHgj/jr9daLe4jbVRCH0FlEP8bIQwr/isv8H3CJpPdG++VWedi4nGsqYQLQfVwDXUsB+DCEsIzpY+B2i5/g3oF9ilkeJDiQ+Er+OctUViN4fn6bhhztEBxv7EwVhZVxvNjOJDqK+AfyexDeEEEId0fBZf6JjIjVEw0t7xrMMBf4Zb7spwAXx+3S3pYbfiiwtkr4PHBpCyDytx1Km6HTE/wohfGZXtyWfeOhsGTA6tLIfDFlhWsOJ3LudeDw0s6dlVojzicZi/7SrG2I75pM4vPCJJulyomGDp0MIf97V7bFPDkU/O/8J0Sly/oqaAkn3SXpX0qtZyiXpJ5KqFP2o6ti8dXrfmZk1TdEvCTcAs0MIRzVRPpToYP5Qoh9dTQ0hnJg5X5J7umZmWcTfRt/PMcswokAOIYSFwN7xGR5ZtfiYriR3pc2sICGEgs65zldNoTMqui7KmMRD00MI05uxru40/GFMdfzYW9kW8IE0M9ttxQHbnJDN1NSHRM7Qd+iaWUlpznGqhj9e3SHVNPyFbA8+/iVjkzyma2YlZevWrQXfiqCS6PoWin+tuC7+lWZW7umaWUkp5hlZkn5JdMW9ckXXl9522VBCCNOIrqsylOjXrZuILvCTu86WPmXMB9Jsm3322YeJEyfSu3dv2rTxl6zd1datW6mqqmLixImsWbOmQVkxDqRt2bKl4Mxp165dMQ7cNYtD11IzdepU+vfvT9u2/oK1u6urq+Oll17i6quvbvB4MUK3tra24Mxp37596qHrV7+lpnfv3g5cA6Bt27b07t27Repu7T/48jvAUuMhBUtqqdeDQ9esCeW/+U3+mXZQzXnntVjd1voV6ayEFuOuh+0W1q5dy8iRIxk5ciRnnnkmQ4cO3X5/y5Yt+SsAfvCDH7B8+fKitOfss89m/fr1TT4+fPhwRowYwVVXXcX770e/QL3qqqvYuHFjs9dTV1fHqaee2qxl3nvvPU466SSeeCLfNdJbp+b8X9mu4J6u7Rb23ntv5syZA8D06dPp1KkTF13U8Mqa2/84MMvX3ptuuqnF2wkwY8YMunbtyk9+8hNmzZrFNddcw09/+tNU1g3wzDPP0K9fP+bOncuwYcOanKe+vp6ysrLU2tQcHl4wy6MYwwE7OlyxcuVKrrvuOj73uc/x6quvcueddzJjxgyWLFnC5s2bqaio4PLLLwfgsssu4/rrr6dXr15UVFRw3nnnsWDBAjp27MiUKVPYd999ee+997j11lt55513kMR1111Hv379WLNmDTfeeCPr1q3jqKOOKigYjjnmGB577DEg6gE/9NBDLFu2jFtvvZVZs2axZcsWRo8ezW233UbPnj2ZOXMm8+fPp7a2ltNOO217u7d59913ueGGG9i0aRP19fXccMMNfPazn2203nnz5nH99dczfvx4ampqKC8vp66ujoqKCs4//3wWLlzItddeS1lZGVOnTuXDDz9kn3324aabbmK//fbj0Ucf5YknnqCuro6DDz6YiRMn0rFjen+L1tpD18MLtttbtmwZ5557Lg8++CDdunVj7NixzJ49mzlz5vDSSy+xdOnSRsts2LCBY489ljlz5tCvXz8qK6N/ZJoyZQqjRo1i9uzZ3HLLLUyaNAmAe+65h+OPP54HHniAAQMGsHr16pxtCiHwwgsvNDrCf/TRRzNgwACmTZvGj3/8Y8455xx69uzJiy++yNtvv83MmTN58MEHeeWVV/jHP/7RYNmnn36aU045hTlz5jBnzhz69OnTaL2rVq3igw8+4IgjjmDw4ME8++zHfw69YcMGDjvsMGbNmsXhhx/OHXfcweTJk7n//vsZMmQI06ZNA2Dw4MHbt1/37t158snm/KPSzvPwglkr16NHD4488uM/L547dy6VlZXU19ezevVqli1bxiGHHNJgmQ4dOnDyyScDcPjhh/Pyyy8DsGjRIt54443t861fv57Nmzfz97//nalTpwIwcOBAunTpkrU9l19+OW3atOHQQw9l1KhRjcqvuOIKLrroIrp06cL48eMBWLhwIQsWLOBrX4v+tu7DDz9kxYoVDZ5X3759ueWWW6itrWXgwIEceuihjeqeO3cuFRUVAJxxxhlMnjyZ4cOj/+Fs167d9vHhZcuWsXTpUq688kogOnjVrVs3AKqqqrjnnntYv349mzZt4gtf+ELW59oSWvuBNIeu7faSX31XrFjBww8/zMyZM+natSsTJkzgo48+arRMu3bttk+XlZVRXx/9sXAIgVmzZjUob65tY7rZrF27ls2bNwNQW1tLx44dCSFw6aWXNhqDraur2z59wgknMG3aNF544QUmTJjA6NGjGTJkSIP5582bx7p16/jd734HwOrVq3nzzTfZf//96dChQ4MLxPTp04cZM2Y0at9NN93E1KlT6d27N48//jivvtrkny60mNY+vODQtV2uJU8fa66NGzfSuXNnunTpQk1NDQsXLmTAgAEFL9+/f39+/etfM3LkSACWLFnCYYcdxjHHHMPvf/97Ro8ezZ///OcdOhNhmx/96EeMHTuW5cuXc/fdd3PttdcyYMAA7r33Xs444ww6derEO++8Q4cOHdhjjz22L/fWW2/RrVs3zjvvPDZu3MiSJUsahO7SpUupr6/nqaee2v7Yz372M+bNm9fooGPPnj159913ee211zjyyCPZsmULK1asoFevXmzevHn7OPDcuXPp3r37Dj/XHeHQNfsEOfzww+nZsyfDhw+ne/fuTR5oymXcuHHceuut/Pa3v6W+vp7jjjuO73znO1xxxRV873vf49lnn+W4447b/lW8uSorK+nUqRMVFRXU1dVx6aWX8te//pWTTz6Z5cuXc8kl0fVWOnfuzKRJkxqE7qJFi3jwwQdp27YtnTt35uabb25Q99y5cxudXnbaaacxceLERqHbvn17brvtNqZMmcKmTZuoq6vjwgsvpFevXlxxxRVcfPHFHHDAAfTq1Yva2nT/cb21h66vvWCpefrppykvLwf84wiDmpqaRsMbxbj2Qk1NTcGZU15e7msv2O7BwWgtpbX3dB26ZlZSfPaCWay1vxksXS31emjtPV3/OMJSU1VV1eAUJtt91dXVUVVV1SJ1t/YfR/hAmqXG/xxh0PL/HLFq1aqCM+fAAw/0P0eY2e6rGKH75ptvFpw53bt399kLZmY7o7UfO3DomllJae0H0hy6ZlZSHLpmZily6JqZpciha2aWIoeumVmKfPaCmVmK3NM1M0uRQ9fMLEUOXTOzFDl0zcxS5NA1M0uRz14wM0uRe7pmZily6JqZpai1h64v329mJaWYf9cj6SxJSyRVSRrfRPnBkuZL+rukVyQNzVene7pmVlKKdSBNUhlwN1ABVAOLJFWGEF5PzHYj8KsQws8l9QWeAj6Tq173dM2spBSxp9sfqAohLA0h1AIPAcMyVwfsGU/vBazKV6l7umZWUpozpitpDDAm8dD0EML0eLo7sDJRVg2cmFHFRGCepKuALsDp+dbp0DWzktKc0I0DdnqW4qb+tDKz8hHAzBDCHZIGAPdLOiqEkHWMw6FrZiWliGcvVAMHJe73oPHwwdeBs+L1LpDUESgH3s1Wqcd0zaykFHFMdxHQR1JPSe2B4UBlxjwrgMEAko4AOgKrc1Xqnq6ZlZRinb0QQqiTNBaYC5QB94UQXpN0M7A4hFAJXAvMkHQN0dDD6JAnzdXSJxJLat1nKptZqxFCaGoctVkWLFhQcOYMGDBgp9fXXO7pmllJae2/SHPomllJceiamaXIoWtmliKHrplZinwRczOzFLmna2aWIoeumVmKHLpmZily6JqZpcgH0szMUuSerhXsur32ylm+bt26VNrRrm3ul8WWurpU2rGzvnnllTnL7/7Zz1q8DacPHpx3nmf/8AdmtHhLdh8OXTOzFDl0zcxS5NC1HfJg586NHnsrpeGFDmVlOcs/+oQML3zpzDNzls9IYXhhj6OPzlp20iuvtPj6d0cOXTOzFPnsBTOzFLmna2aWIoeuFSx5Slha47dN+eijj3bZuotp2LBhu7oJ3HnnnVnLLk+xHbsTh66ZWYocumZmKXLompmlyGcvmJmlyD1dM7MUOXTNzFLk0LWCDTv33O3TB59wQqPyCRMm5K1j6NChOcufeuqpvHVUV1fnLO/Ro0fO8j/96U951zFw4MCc5YccckjO8qVLl+Zdx9tvv52z/IADDshZPnny5LzrGDduXM7yKVOmZC3rt3AhAI888kje9VjhHLpmZinygTQzsxS5p2tmliKHrplZihy6ZmYpau2hq5ZuoKTWvQVakeQFUPyfWaXP+7uxEIJ2to7Zs2cXnDmjRo3a6fU1l3u6ZlZSfPaCmVmKWvvwQptd3QAzs2IKIRR8y0fSWZKWSKqSND7LPOdLel3Sa5Lm5KvTPV0zKynF6ulKKgPuBiqAamCRpMoQwuuJefoA3wVODiGskdQtX73u6ZpZSSliT7c/UBVCWBpCqAUeAjL/juRy4O4Qwpp43e/mq9Sha2YlZevWrQXfJI2RtDhxG5OoqjuwMnG/On4s6VDgUEkvSloo6ax87fPwgjWyevXqnOWf+tSncpbPnz8/7zpOPfXUnOW//OUvc5aPGDEi7zo2btyYs7xLly45y48//vi861i8eHHO8sMOOyxr2QFr1wLw9jvv5F2PFa45wwshhOnA9CzFTZ1Olll5W6APMAjoATwv6agQwtps63RP18xKShGHF6qBgxL3ewCrmpjniRDClhDCMmAJUQhn5dA1s5JSxNBdBPSR1FNSe2A4UJkxz+PAqQCSyomGG3Jed9TDC2ZWUop19kIIoU7SWGAuUAbcF0J4TdLNwOIQQmVcdoak14F64PoQwnu56nXomllJKeaPI0IITwFPZTz2/cR0AL4d3wri0DWzkuKfAZuZpai1/wzYoWuN5DslLJ98p4MVopBTwvLJd0pYPvlOByvEkiVLspbl/gc321EOXTOzFDl0zcxS5NA1M0uRQ9fMLEU+e8HMLEXu6ZqZpcihawW74IILtk8f9fnPNyq/+uqr89axfv36nOVdu3bNW0dNTU3O8vLy8pzlgwYNyruO5557Lu88u1p9fX3eecrKynKWr1qVeX2Uj3V64AEAxo0b17yGWU4OXTOzFDl0zcxS5ANpZmYpck/XzCxFDl0zsxQ5dM3MUuTQtYI9/PDD26dnJKabo5BTwvLJd0pYPp+E08EKke90sEIceOCBWcsu3+narSkOXTOzFPnsBTOzFLmna2aWIoeumVmKHLpmZily6FrBinHBm9ra2pzl7du3z1vHPvvsk7N8zZo1OcvvuuuuvOsYO3ZszvLbb789Z/n111+fdx2nnHJKzvLnn38+Z/mGDRvyrmOPPfbY4XXsX1kJ5H+u1jwOXTOzFPnsBTOzFLmna2aWIoeumVmKHLpmZily6JqZpai1H0hTS38qSGrdHzutSPICKDN2WSssLd7fjYUQtLN1XHPNNQVnzp133rnT62su93TNrKR4eMHMLEUOXTOzFDl0zcxS5NA1M0tRaz97oc2uboCZWTGFEAq+5SPpLElLJFVJGp9jvq9ICpKOz1ene7olJt+nfJs2+T9n6+rqcpa3bdvyL5tBgwblLC/kf9huvPHGnOWTJk3KWZ5vO0D+bdG3b9+sZQfGV2tb9dZbeddjhSvW8IKkMuBuoAKoBhZJqgwhvJ4xX1fgW8BfCqnXPV0zKylF7On2B6pCCEtDCLXAQ8CwJub7ITAZ2FxI+xy6ZlZSmhO6ksZIWpy4jUlU1R1YmbhfHT+2naRjgINCCE8W2j4PL5hZSWnO8EIIYTowPUtxU79W2165pDbAncDoZjTPoWtmpaWIZy9UAwcl7vcAViXudwWOAp6TBHAAUCnp3BDC4myVOnTNrKQU8TzdRUAfST2BN4HhwMjEetYB5dvuS3oOuC5X4IJD18xKTLFCN4RQJ2ksMBcoA+4LIbwm6WZgcQihckfq9VXGWhFfdWr34v3dWDGuMnbJJZcUnDm/+MUvfJUxM7Od4Z8Bm5mlqLX/DNiha2YlxT1dM7MUOXTNzFLk0DUzS5FD1wrW94gjtk9X9OjRqPyZZ57JW8e4ceNylk+ePDlvHbW1tTnL27dvn7eOnZXv6l2FXAGspqYmZ3l5eXnO8j/+8Y9513HaaaflLL/99tuzlh29cCEAjzz6aN71WOEcumZmKfLZC2ZmKXJP18wsRQ5dM7MUOXTNzFLU2kPXF7xpRXwBlN2L93djxbjgzZe//OWCM+exxx7zBW/MzHZGa+/pOnTNrKQ4dM3MUuTQNTNLkUPXzCxFDl0zsxT5Z8BWsCvGjNk+fUZFRaPyr371q3nrqGhiuaRCLppTKsaOHZuz/K677mrxNvTq1Str2ac++ACA1atXt3g7difu6ZqZpciha2aWIoeumVmKHLpmZily6JqZpchnL9gO6dnEWQaXNzFfpr7V1TnLP7NjzflE+vyrr+Ys/yiFNmw7Q8HS09p7ur7KWCtSSKhaafJVxiLFuMrYqaeeWnDmzJ8/31cZMzPbGa29p+vQbUXc2zHbeQ5dM7MU+UCamVmK3NM1M0uRQ9fMLEUOXTOzFLX20G2zqxtgZlZMIYSCb/lIOkvSEklVksY3Uf5tSa9LekXSHyR9Ol+dDl0zKylbt24t+JaLpDLgbmAI0BcYIalvxmx/B44PIRwNPAJMztc+h66ZlZQi9nT7A1UhhKUhhFrgIWBYxrrmhxA2xXcXAj3yVerQNbOS0pzQlTRG0uLEbUyiqu7AysT96vixbL4OPJ2vfT6QZmYlpTkH0kII04HpWYqbui5Dk5VLuhA4HhiYb50OXTMrKUU8e6EaOChxvwewKnMmSacD3wMGhhDyXrzOoWtmJaWIobsI6COpJ/AmMBwYmZxB0jHAPcBZIYR3C6nUoWtmJaVY114IIdRJGgvMBcqA+0IIr0m6GVgcQqgEbgf2AH4tCWBFCOHcXPX6erpm1moU43q6Rx99dMGZ88orr/h6umZmO6O1/yLNoWtmJcWha2aWIoeumVmKfBFzM7MUuadrZpYih66ZWYocumZmKXLompmlyKFrZpYin71gZpYi93TNzFLk0DUzS5FD18wsRQ5dM7MUOXTNzFLksxfMzFLknq6ZWYocumZmKXLompmlyKFrZpYiH0gzM0uRe7pmZily6JqZpciha2aWIoeumVmKHLpmZiny2QtmZilyT9fMLEUOXTOzFDl0zcxS5NA1M0uRQ9fMLEU+e8HMLEXu6ZqZpai1h26bXd0AM7NiCiEUfMtH0lmSlkiqkjS+ifIOkh6Oy/8i6TP56nTomllJKVboSioD7gaGAH2BEZL6Zsz2dWBNCKE3cCdwW772OXTNrKRs3bq14Fse/YGqEMLSEEIt8BAwLGOeYcCsePoRYLAk5aq0xcd0Qwg5G2BmVkzNyRxJY4AxiYemhxCmx9PdgZWJsmrgxIwqts8TQqiTtA7YD6jJtk4fSDOz3VYcsNOzFDcV3pljEoXM04CHF8zMmlYNHJS43wNYlW0eSW2BvYD3c1Xq0DUza9oioI+knpLaA8OByox5KoGL4+mvAH8MeY7QeXjBzKwJ8RjtWGAuUAbcF0J4TdLNwOIQQiVwL3C/pCqiHu7wfPWqtZ9IbGZWSjy8YGaWIoeumVmKHLpmZily6JqZpciha2aWIoeumVmKHLpmZily6JqZpciha2aWIoeumVmKHLpmZily6JqZpciha2aWIoeumVmKHLpmZily6JqZpciha2aWIoeumVmK/j9DvZ+TejAKKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(X[0], cmap='gray')\n",
    "ax.set(title='Input Image')\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "for i, layer in enumerate(clf.hidden_layers):              \n",
    "    ax = sns.heatmap(layer.pixel_values, vmin=0, vmax=1, cmap='gray')\n",
    "    trained_pixels = clf.grid_W_map[i].keys() \n",
    "\n",
    "    min_extent = clf.layer_shape\n",
    "    max_extent = (0, 0)\n",
    "\n",
    "    for grid_coord in trained_pixels:\n",
    "        grid_row, grid_col = grid_coord\n",
    "\n",
    "        if grid_row < min_extent[0]:\n",
    "            min_extent = (grid_row, min_extent[1])\n",
    "\n",
    "        if grid_row > max_extent[0]:\n",
    "            max_extent = (grid_row, max_extent[1])\n",
    "\n",
    "        if grid_col < min_extent[1]:\n",
    "            min_extent = (min_extent[0], grid_col)\n",
    "\n",
    "        if grid_col > max_extent[1]:\n",
    "            max_extent = (max_extent[0], grid_col)\n",
    "\n",
    "    height = (max_extent[0] - min_extent[0]) + 0.9\n",
    "    width = (max_extent[1] - min_extent[1]) + 0.9\n",
    "\n",
    "    rect = patches.Rectangle((min_extent[1], min_extent[0]), width, height, \n",
    "                             linewidth=3, alpha=0.4, \n",
    "                             edgecolor='r', facecolor='none',\n",
    "                             label='Trained Pixels Area')\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.set(title='Hidden Layer %d Pixel Transparency Values' % i)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "        \n",
    "ax = sns.heatmap(clf.predict_proba(X).detach().numpy()[0].reshape(1, -1), vmin=0, cmap='gray')\n",
    "ax.set(title='Output')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
